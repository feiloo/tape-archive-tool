#!/usr/bin/python3.12
import argparse
import os
import sys
import subprocess
import re
from typing import List
import tempfile
import stat
from pathlib import Path
from uuid import uuid4
import json
import datetime
import socket

# this tool is very very strict to remove as many error cases as possible
# it tracks data more redundantly (possibly inconsistently) to ensure high availability and partition tolerance

# a main problem with tape archiving is that random access to tape archives is very (time) costly
# so a main point of this tool is also, to prevent archiving small files that more likely need random access from being archived and left with only the slow tape-access

# this tool is specifically built to archive only few but very big files
# we do not want any versioning of our big files. we want them archived as objects, flatly

STUBFILE_SUFFIX = ".archive_stub"

MIN_FILESIZE_BYTES = 1_000_000_000 # 1 GB
MAX_FILENAME_LEN = 255
MAX_DEPTH = 16
MAX_TOTAL_LEN = 512

session_uuid = (uuid4())
session_hostname = socket.getfqdn()
session_hosttime = str(datetime.datetime.now().isoformat())

def subproc(cmd: List[str]):
    ''' subprocess wrap function for better monkeypatching and better argument control '''
    result = subprocess.run(
        cmd,
        check=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        encoding='utf-8',
        errors='strict'
    )
    return result

# AI-generated by Qwen3-235B-A22B-Instruct-2507

def parse_file_space_names(text: str) -> List[str]:
    """
    Parse the given text and return a list of file space names using regex.
    Fail hard on any parsing issue.
    
    Args:
        text (str): Input text containing file space table.
        
    Returns:
        List[str]: List of file space names.
        
    Raises:
        ValueError: If parsing fails at any stage.
    """
    if not text:
        raise ValueError("Input text is empty")
    
    lines = text.strip().splitlines()
    if len(lines) < 3:
        raise ValueError("Input too short to contain table")

    # Find header row with column names
    header_pattern = r"^\s*#\s+Last\s+Incr\s+Date\s+Type\s+File\s+Space\s+Name\s*$"
    header_idx = None
    for i, line in enumerate(lines):
        if re.match(header_pattern, line):
            header_idx = i
            break
    if header_idx is None:
        raise ValueError("Header not found")

    # Next line must be separator (dashes)
    if header_idx + 1 >= len(lines) or not re.match(r"^-+$", lines[header_idx + 1].strip()):
        raise ValueError("Separator line not found or invalid")

    # Parse data rows
    file_spaces = []
    # Match lines starting with number, then date, then type, then path
    data_pattern = r"^\s*\d+\s+\d{2}/\d{2}/\d\s+\d{2}:\d{2}:\d{2}\s+([a-zA-Z0-9]+)\s+(/\S*)\s*$"
    
    for i in range(header_idx + 2, len(lines)):
        line = lines[i].strip()
        if not line:
            continue
        match = re.match(data_pattern, lines[i])
        if not match:
            raise ValueError(f"Failed to parse line {i+1}: '{lines[i]}'")
        file_space_name = match.group(2)
        file_spaces.append(file_space_name)
    
    if not file_spaces:
        raise ValueError("No data rows found")
    
    return file_spaces


def validate_filepath(filepath: str, ) -> None:
    """
    Validate filepath for length, depth, and character safety.
    Raises ValueError if invalid.
    """
    if not filepath:
        raise ValueError("File path is empty")

    # Check total path length
    if len(filepath) > MAX_TOTAL_LEN:
        raise ValueError(f"Path too long: {len(filepath)} > {MAX_TOTAL_LEN} characters")

    # Check depth (number of directories)
    parts = [p for p in filepath.split(os.sep) if p]
    if len(parts) > MAX_DEPTH:
        raise ValueError(f"Path too deep: {len(parts)} components > {MAX_DEPTH}")

    # Extract filename
    filename = os.path.basename(filepath)
    if len(filename) > MAX_FILENAME_LEN:
        raise ValueError(f"Filename too long: {len(filename)} > {MAX_FILENAME_LEN} characters")

    # Check for non-ASCII characters
    try:
        filename.encode('ascii')
    except UnicodeEncodeError:
        raise ValueError("Filename contains non-ASCII characters")

    # Check for unsafe characters (allow letters, digits, underscore, dash, dot)
    if not re.fullmatch(r'[a-zA-Z0-9._-]+', filename):
        raise ValueError("Filename contains invalid characters (allowed: a-z, A-Z, 0-9, _, -, .)")

    # Check for null bytes or control chars
    if '\x00' in filepath or any(ord(c) < 32 for c in filepath):
        raise ValueError("Path contains null or control characters")

    # Reserved names on some systems (Windows)
    reserved = {'CON', 'PRN', 'AUX', 'NUL', 'COM1', 'LPT1'}
    if filename.split('.')[0].upper() in reserved:
        raise ValueError(f"Filename '{filename}' is a reserved name")


def is_normal_file(filepath: str) -> bool:
    """
    Check if file is 'normal': exists, regular file, not symlink, not device, not broken.
    """
    try:
        # Resolve symlinks and get final stat
        #real_path = os.path.realpath(filepath)
        real_path = os.path.realpath(filepath)
        st = os.stat(real_path)

        # Must be a regular file
        if not stat.S_ISREG(st.st_mode):
            return False

        # Path must still point to same inode (prevent symlink race)
        st_final = os.stat(filepath)
        if st.st_ino != st_final.st_ino or st.st_dev != st_final.st_dev:
            return False

        return True

    except (OSError, ValueError):
        return False


def sha256sum(filepath: str) -> str:
    """Compute SHA256 sum of a file using the `sha256sum` subprocess to ensure compatibilty with users using the command."""
    try:
        result = subproc(['sha256sum', filepath])
        # Output: "<hex>  <filename>"
        checksum = result.stdout.split(None, 1)[0]
        return checksum.lower()
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"sha256sum failed: {e.stderr.strip()}") from e
    except FileNotFoundError:
        raise RuntimeError("sha256sum utility not found")
    except Exception as e:
        raise RuntimeError(f"Unexpected error: {e}") from e

# end generated by AI

def stubname(path):
    return path + STUBFILE_SUFFIX

def get_object_metadata(path):
    """ optional, calculate additional metadata for the object here """
    return {'secondary_obj_id':str(uuid4())}


def archiving_pre_check(paths):
    # preflight checks:
    if list(set(paths)) != list(paths):
        raise RuntimeError(f'object can only be specified and archived once')

    for path in paths:
        if not Path(path).exists():
            raise RuntimeError(f"cant archive nonexistent file: {path}")
        if not is_normal_file(path):
            raise RuntimeError(f'error, refusing special files: {path}')
        if get_filesize(path) < MIN_FILESIZE_BYTES:
            raise RuntimeError(f'error, refusing to archive small files to tape: {path}')

        if Path(stubname(path)).exists():
            raise RuntimeError(f'''error, archive stubfile: {stubname(path)} already exists, the file likely has already been archived
                                refusing to archive again''')
        if not Path(path).is_file():
            raise RuntimeError(f"object path is not a file, use '-r' with caution to archive directories")

        validate_filepath(path)


def archive_objects(paths, dry_run=False):
    if not isinstance(paths, list): raise RuntimeError('paths must be list')
    archiving_pre_check(paths)
            
    stubfiles = []
    for path in paths:
        print(f"Archiving {path}")
        file_checksum = sha256sum(path)
        metadata = get_object_metadata(path)
        if not dry_run:
            with open(path + '.archive_stub', 'wt') as f:
                f.write(json.dumps({"entry_type":"pre_archive_check", "path":str(path), "sha256checksum":file_checksum, "metadata": metadata}) + '\n')
            stubfiles.append(stubname(path))


    # use a filelist to batch archive, this uses a single session instead of closing and opening as a plain loop would do
    # importantly, sort the files, such that they're possibly more efficiently written
    filelist = list(sorted(paths + stubfiles))
    with tempfile.NamedTemporaryFile(mode='w+', suffix='.txt', prefix='archive_upload_filelist_') as tmpfile:
        tmpfile.write('\n'.join(filelist))
        tmpfile.seek(0)

        if dry_run:
            cmd = ['dsmc', 'preview', 'archive', f'-filelist={tmpfile.name}', '-changingretries=0', '-filesonly=yes']
        else:
            cmd = ['dsmc', 'archive', f'-filelist={tmpfile.name}', '-changingretries=0', '-filesonly=yes']

        try:
            #subprocess.run(cmd, check=True)
            subproc(cmd)
        except subprocess.CalledProcessError as e:
            print(f"Archive operation failed: {e}")
            sys.exit(1)

    for path in paths:
        if not dry_run:
            with open(path + '.archive_stub', 'at') as f:
                f.write(json.dumps({'entry_type':"state", "state":"successfully_archived", "path":str(path)}) + '\n')
        print(f"successfully archived {path}")

    for path in paths:
        Path(path).unlink(missing_ok=False)
        print(f"successfully removed {path}")


def archive_recursively(path):
    raise NotImplementedError()


def get_filesize(path) -> int:
    stat = os.stat(path) # returns size in bytes
    return int(stat.st_size)

def get_all_filespaces():
    try:
        result = subproc(['dsmc', 'query', 'filespace'])
        print(result.stdout)
        return parse_file_space_names(str(result.stdout))
    except subprocess.CalledProcessError as e:
        print(f"Query filespaces operation failed: {e}")
        sys.exit(1)

def list_archived_objects_under_path(path, ignore_error=False):
    try:
        subproc(['dsmc', 'query', 'archive', '-subdir=yes', path])
    except subprocess.CalledProcessError as e:
        print(f"Query archive operation failed: {e}")
        if ignore_error:
            pass
        else:
            sys.exit(1)

def list_archived_objects(paths, ignore_missing):
    print("Listing archived objects")

    # no arguments will to list all paths
    if paths == []:
        # get all filespaces (filesystem paths):
        filespaces = get_all_filespaces()

        for p in filespaces:
            list_archived_objects_under_path(p.removesuffix('/') + '/*', ignore_error=ignore_missing)
    else:
        for p in paths:
            list_archived_objects_under_path(p)

def get_from_archive(name, destination):
    print(f"getting {name} to {destination}")
    if name.endswith('.archive_stub'):
        raise RuntimeError(f"name looks like a stubfile, use the actual objectname instead")
    if not Path(stubname(name)).exists():
        raise RuntimeError(f"stubfile {stubname(name)} for {name} not found")

    if Path(destination).exists():
        raise RuntimeError("destination path is not free")

    try:
        subproc(['dsmc', 'retrieve', '-replace=no', name, destination])
        print(f"{name} successfully retrieved")
    except subprocess.CalledProcessError as e:
        print(f"Retrieving archive operation failed: {e}")
        sys.exit(1)

def retrieve_object(name, destination):
    if (stubname(name) == destination) or (stubname(name) == stubname(destination)):
        raise RuntimeError("retrieveing a object to its original path is not supported. use recall to move a file from the archive back to its original path")
    # todo open stubfile, to verify hash after retrieve
    get_from_archive(name, destination)

def recall(name):
    get_from_archive(name, name)
    with Path(stubname(name)).open('at') as f:
        f.write(json.dumps({'entry_type':"state", "state":"started_unarchiving", "path":str(name)}) + '\n')
    delete_object(name)
    Path(stubname(name)).unlink(missing_ok=False)


def delete_object(name):
    print(f"Deleting {name} from archives")
    if name.endswith('/'):
        print(f"refusing to plainly delete archive directory, use '-r' with caution for directories")
        sys.exit(1)

    try:
        subproc(['dsmc', 'delete', 'archive', '-noprompt', name])
        print(f"{name} successfully deleted")
    except subprocess.CalledProcessError as e:
        print(f"Deleting archive operation failed: {e}")
        sys.exit(1)


def print_info():
    ''' print info about the IBM Storage Protect system '''
    print(f"getting systeminfo")
    try:
        with tempfile.NamedTemporaryFile(mode='w+', suffix='.log', prefix='app_', delete=True) as f:
            result = subproc(['dsmc', 'query','systeminfo', f'-filename={f.name}'])
            print(f"got syteminfo to {f.name}")
            f.seek(0)
            print(f.read())
            
    except subprocess.CalledProcessError as e:
        print(f"querying systeminfo failed: {e}")
        sys.exit(1)

    print(result.stdout)


def main():
    parser = argparse.ArgumentParser(description='Archive system client utility')
    subparsers = parser.add_subparsers(dest='command', required=True)

    # List command
    list_parser = subparsers.add_parser('list', help='List all archived objects or all in the given paths')
    list_parser.add_argument('path', type=str, nargs='*', help='multiple paths prefixes to list list archived objects of, empty lists all archived objects')
    list_parser.add_argument('--ignore-missing', dest='ignore_missing', action='store_true', help='continues listing, even if a path doesnt exist and errors occur')

    # Archive command
    archive_parser = subparsers.add_parser('archive', help='Migrate files to the archive system')
    archive_parser.add_argument('--dry-run', '-n', action='store_true', dest='dry_run', help='dryrun the archiving')
    archive_parser.add_argument('object_path', nargs="+", type=str, help='Path to the file/folder to archive')

    # Retrieve command
    retrieve_parser = subparsers.add_parser('retrieve', help='Retrieve a copy of an archived object')
    retrieve_parser.add_argument('object_name', type=str, help='Name of archived object (without extension)')
    retrieve_parser.add_argument('--destination', '-d', type=str, default=os.getcwd(),
                                help='Target directory for retrieval (default: current directory)')
    # recall command
    recall_parser = subparsers.add_parser('recall', help='Migrate an archived object back to its original path')
    recall_parser.add_argument('object_name', type=str, help='Name of archived object (without extension)')

    # Delete command
    delete_parser = subparsers.add_parser('delete', help='Remove an object from the archives')
    delete_parser.add_argument('object_name', type=str, help='Name of the archived object to delete')

    # Info command
    info_parser = subparsers.add_parser('info', help='Print archive system information')

    args = parser.parse_args()

    if args.command == 'list':
        list_archived_objects(args.path, args.ignore_missing)
    elif args.command == 'archive':
        archive_objects(args.object_path, args.dry_run)
    elif args.command == 'retrieve':
        retrieve_object(args.object_name, args.destination)
    elif args.command == 'recall':
        recall(args.object_name)
    elif args.command == 'delete':
        delete_object(args.object_name)
    elif args.command == 'info':
        print_info()
    else:
        parser.print_help()

if __name__ == '__main__':
    main()
